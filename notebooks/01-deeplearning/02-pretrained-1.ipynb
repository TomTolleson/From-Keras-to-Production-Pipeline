{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /root/.keras/keras.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# image processing\n",
    "from keras.preprocessing import image as image_utils\n",
    "\n",
    "# pretrained nets\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files_path = \"/keras2production/fruits/Training/\"\n",
    "valid_image_files_path = \"/keras2production/fruits/Test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained nets\n",
    "\n",
    "https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use VGG16\n",
    "\n",
    "https://keras.io/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg = VGG16(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image):\n",
    "    img = cv2.imread(image)        \n",
    "    b,g,r = cv2.split(img)       # get b,g,r\n",
    "    img = cv2.merge([r,g,b])     # switch it to rgb\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    # images need to be numpy arrays of 224 x 224 color images with 3 channels\n",
    "    image = image_utils.load_img(image, target_size=(224, 224))\n",
    "    image = image_utils.img_to_array(image)\n",
    "\n",
    "    # expand the dimensions to be (1, 224, 224, 3)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # scale pixels between -1 and 1, sample-wise\n",
    "    image = preprocess_input(image)\n",
    "        \n",
    "    preds = model_vgg.predict(image)\n",
    "    res = decode_predictions(preds, top=3)\n",
    "    \n",
    "    for (i, (imagenetID, label, prob)) in enumerate(res[0]):\n",
    "        print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = !find $train_image_files_path -type f -name \"*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "train_images_r = random.sample(train_images, num)\n",
    "for i in range(num):\n",
    "    classify_image(train_images_r[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = '/keras2production/notebooks/01-deeplearning/test_images/Banana/Banana_wiki.jpg'\n",
    "img = image_utils.load_img(image, target_size=(224, 224))\n",
    "# `x` is a float32 Numpy array of shape (224, 224, 3)\n",
    "x = image_utils.img_to_array(img)\n",
    "\n",
    "# We add a dimension to transform our array into a \"batch\"\n",
    "# of size (1, 224, 224, 3)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Finally we preprocess the batch\n",
    "# (this does channel-wise color normalization)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model_vgg.predict(x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_output = model_vgg.output[:, np.argmax(preds[0])]\n",
    "banana_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The is the output feature map of the `block5_conv3` layer,\n",
    "# the last convolutional layer in VGGG16\n",
    "last_conv_layer = model_vgg.get_layer('block5_conv3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the gradient of the class with regard to\n",
    "# the output feature map of `block5_conv3`\n",
    "grads = K.gradients(banana_output, last_conv_layer.output)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a vector where each entry\n",
    "# is the mean intensity of the gradient over a specific feature map channel\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function allows us to access the values of the quantities we just defined:\n",
    "# `pooled_grads` and the output feature map of `block5_conv3`,\n",
    "# given a sample image\n",
    "iterate = K.function([model_vgg.input], [pooled_grads, last_conv_layer.output[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the values of these two quantities, as Numpy arrays,\n",
    "# given our sample image \n",
    "pooled_grads_value, conv_layer_output_value = iterate([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We multiply each channel in the feature map array\n",
    "# by \"how important this channel is\" \n",
    "for i in range(512):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The channel-wise mean of the resulting feature map\n",
    "# is our heatmap of class activation\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "heatmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = np.maximum(heatmap, 0)\n",
    "heatmap /= np.mean(heatmap)\n",
    "heatmap = np.uint8(heatmap * -0.6)\n",
    "plt.imshow(heatmap, cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cv2 to load the original image\n",
    "img = cv2.imread(image)\n",
    "b,g,r = cv2.split(img)       # get b,g,r\n",
    "img = cv2.merge([r,g,b])     # switch it to rgb\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resize the heatmap to have the same size as the original image\n",
    "heatmap2 = cv2.resize(heatmap, (img.shape[1], img.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(heatmap2, cmap=\"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap3 = cv2.applyColorMap(heatmap2, cv2.COLORMAP_HOT)\n",
    "\n",
    "# 40 here is a heatmap intensity factor\n",
    "superimposed_img = heatmap3 * 40 + img\n",
    "\n",
    "plt.imshow(superimposed_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.zeros_like(img)\n",
    "\n",
    "for i in range(out.shape[2]):\n",
    "    for j in range(out.shape[0]):\n",
    "        for k in range(out.shape[1]):\n",
    "            if heatmap2[j,k] == 0:\n",
    "                out[j,k,i] = 0\n",
    "            else:\n",
    "                out[j,k,i] = img[j,k,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
