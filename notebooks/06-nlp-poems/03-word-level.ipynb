{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Classifying poems as word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EXTRACT = 'selected_poems.json.bz2'\n",
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyzäöüßABCDEFGHIKLMNOPQRSTUVWXZYÄÖÜ .,;:!?-()\"\\'\\n'\n",
    "\n",
    "def clean_text(text):\n",
    "    return ''.join([char for char in text if char in ALPHABET])\n",
    "\n",
    "poems = pd.read_json(EXTRACT, compression='infer')\n",
    "poems['cleaned_text'] = poems.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode poems as word sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the poems numerically as **sequences of words** instead of characters, we need to\n",
    "\n",
    "1. split up each poem into sequences of words or token,\n",
    "2. determine the vocabulary to enumerate the token,\n",
    "3. replace each token in each poem by its index in the vocabulary.\n",
    "\n",
    "These routine preprocessing tasks can be delegated to libraries like [Keras](https://keras.io) or [scikit-learn](https://scikit-learn.org). Here, we will use the former's text preprocessing class `Tokenizer` which is not well documented in keras itself, but in [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "NUM_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, lower=False) # do not automatically lower-case the entire text\n",
    "tokenizer.fit_on_texts(poems.cleaned_text)\n",
    "poems['word_seq'] = tokenizer.texts_to_sequences(poems.cleaned_text) \n",
    "poems[['cleaned_text', 'word_seq']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the size of the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_words = len(tokenizer.word_docs)\n",
    "nr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the lengths of the poems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "\n",
    "ax = poems['word_seq'].apply(len).plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the convenience function pad_sequences of [Keras](https://keras.io) to trim down the sequences to 300 words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamline remaining preparation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to one-hot-encode the authors and shuffle and split the training data and labels again. To streamline this process, we write a short function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_from_column(column_name, max_len, train_ratio=0.7):\n",
    "    if max_len is None:\n",
    "        X = poems[column_name].values\n",
    "    else:\n",
    "        X = pad_sequences(poems[column_name], max_len)\n",
    "    authors_ohe = pd.get_dummies(poems['author'])\n",
    "    y = authors_ohe.values\n",
    "    short_authors = [author.split(',')[0] for author in authors_ohe.columns]\n",
    "    return train_test_split(X, y, train_size=train_ratio), short_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 300\n",
    "\n",
    "(X_train, X_test, y_train, y_test), authors = data_from_column('word_seq', 300)\n",
    "X_train.shape, y_test.shape, authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dense, Embedding, GlobalMaxPooling1D\n",
    "\n",
    "DIM = 96\n",
    "\n",
    "def build_model(max_len=MAX_LEN):\n",
    "    return Sequential([\n",
    "        Embedding(nr_words+1, DIM, input_shape=((max_len,))),\n",
    "        Conv1D(96, kernel_size=3, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=5, batch_size=8):\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adadelta')\n",
    "    history = model.fit(X_train,y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    return model, pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(build_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "def plot_history(history):\n",
    "    _, (ax1, ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "    history[['loss', 'val_loss']].plot.line(ax=ax1)\n",
    "    history[['acc', 'val_acc']].plot.line(ax=ax2)\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def validate(model):\n",
    "    authors = [author.split(',')[0] for author in pd.get_dummies(poems['author']).columns]\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_res = np.argmax(y_test, axis=1)\n",
    "    print(metrics.classification_report(y_res, y_pred, target_names=authors))\n",
    "    cm = pd.crosstab(y_res, y_pred)\n",
    "    cm.index = authors\n",
    "    cm.columns = authors\n",
    "    print(cm)\n",
    "\n",
    "validate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-basics",
   "language": "python",
   "name": "nlp-basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
