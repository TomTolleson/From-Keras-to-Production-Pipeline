{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick look at spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist a number of python libraries for natural language processing. In this tutorial, we have a look at spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use spaCy, we first need to load a **language model** which contains statistical information about a language, aggregated from the web, in particular wikipedia, or news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned is a **natural language processor** which can be applied to text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Wovon man nicht sprechen kann, darüber muss man schweigen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed text is a sequence of token with linguistic information stored as attributes, for example \n",
    "\n",
    "- the token text,\n",
    "- the part of speech of the token,\n",
    "- the lemma of the text,\n",
    "- an embedding vector.\n",
    "\n",
    "To show these attributes in a table, we use a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({'text': [token.text for token in doc],\n",
    "              'pos': [token.pos_ for token in doc],\n",
    "             'lemma': [token.lemma_ for token in doc],\n",
    "             'vector': [token.vector for token in doc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the part of speach and lemma are almost recognized almost correctly.\n",
    "\n",
    "Moreover, spaCy can be used to \n",
    "\n",
    "- **extract named entities**, \n",
    "- compute **document similarities**,\n",
    "- **parse** the **syntax tree** of text,\n",
    "\n",
    "see the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying poems with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use spaCy's word vectors for our classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the poems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EXTRACT = 'selected_poems.json.bz2'\n",
    "ALPHABET = 'abcdefghijklmnopqrstuvwxyzäöüßABCDEFGHIKLMNOPQRSTUVWXZYÄÖÜ .,;:!?-()\"\\'\\n'\n",
    "\n",
    "def clean_text(text):\n",
    "    return ''.join([char for char in text if char in ALPHABET])\n",
    "\n",
    "poems = pd.read_json(EXTRACT, compression='infer')\n",
    "poems['cleaned_text'] = poems.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we transform the poems into sequences of word vectors. This should not take much more than a minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordvecs(text):\n",
    "    doc = nlp(text)\n",
    "    return np.stack([token.vector for token in doc])\n",
    "\n",
    "with nlp.disable_pipes('parser', 'ner'):\n",
    "    poems['word_vecs'] = [text_to_wordvecs(text) for text in poems['cleaned_text']]\n",
    "\n",
    "poems['word_vecs'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, we disabled the processing stages of spaCy which we did not need to access the word vectors: \n",
    "\n",
    "- parsing the syntax tree and\n",
    "- extraction of named entities.\n",
    "\n",
    "Next, we use the function `data_from_column` from the previous notebook to get our training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def data_from_column(column_name, max_len, train_ratio=0.7):\n",
    "    if max_len is None:\n",
    "        X = poems[column_name].values\n",
    "    else:\n",
    "        X = pad_sequences(poems[column_name], max_len)\n",
    "    authors_ohe = pd.get_dummies(poems['author'])\n",
    "    y = authors_ohe.values\n",
    "    short_authors = [author.split(',')[0] for author in authors_ohe.columns]\n",
    "    return train_test_split(X, y, train_size=train_ratio), short_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "\n",
    "(X_train, X_test, y_train, y_test), authors = data_from_column('word_vecs', MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the model from in the previous notebook, but of course without the embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "DIM = 96\n",
    "\n",
    "def build_model(max_len=MAX_LEN):\n",
    "    return Sequential([\n",
    "        Conv1D(128, kernel_size=3, activation='relu',input_shape=(max_len,DIM)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready, steady, go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs=8, batch_size=8):\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='Adadelta')\n",
    "    history = model.fit(X_train,y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    return model, pd.DataFrame(history.history)\n",
    "\n",
    "model, history = train_model(build_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def validate(model):\n",
    "    authors = [author.split(',')[0] for author in pd.get_dummies(poems['author']).columns]\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_res = np.argmax(y_test, axis=1)\n",
    "    print(metrics.classification_report(y_res, y_pred, target_names=authors))\n",
    "    cm = pd.crosstab(y_res, y_pred)\n",
    "    cm.index = authors\n",
    "    cm.columns = authors\n",
    "    print(cm)\n",
    "\n",
    "validate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fight overfitting, let us increase batch size and try batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, BatchNormalization\n",
    "\n",
    "def build_model(max_len=MAX_LEN):\n",
    "    return Sequential([\n",
    "        Conv1D(128, kernel_size=3,input_shape=(max_len,DIM)),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model, history = train_model(build_model(), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, batch normalization does not really improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a shallow-and-wide model using the functional API of keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to use convolutions of different kernel sizes in parallel to process the input to extract finer and coarser details in parallel. This can be done using the functional API of Keras as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate\n",
    "\n",
    "def convolve_and_pool(units, kernel_size, inputs):\n",
    "    conv = Conv1D(units, kernel_size, activation='relu')(inputs)\n",
    "    return GlobalMaxPooling1D()(conv)\n",
    "\n",
    "def build_model(max_len=MAX_LEN):\n",
    "    inputs = Input((max_len, DIM))\n",
    "    convs = [convolve_and_pool(96, ks, inputs) for ks in (3,5,7)]\n",
    "    concatenated = Concatenate()(convs)\n",
    "    dense = Dense(3, activation='softmax')(concatenated)\n",
    "    return Model(inputs, dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(build_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using convolutions with different kernel sizes, use iterated/stacked convolutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-basics",
   "language": "python",
   "name": "nlp-basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
