{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import *\n",
    "\n",
    "# image processing\n",
    "from keras.preprocessing import image as image_utils\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# build your own nets\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files_path = \"/keras2production/fruits/Training/\"\n",
    "valid_image_files_path = \"/keras2production/fruits/Test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_list = [\"Apricot\", \"Avocado\", \"Banana\", \"Clementine\", \"Cocos\", \"Kiwi\", \"Lemon\", \"Limes\", \n",
    "              \"Mandarine\", \"Orange\", \"Peach\", \"Pineapple\", \"Plum\", \"Pomegranate\", \"Raspberry\", \"Strawberry\"]\n",
    "output_n = len(fruit_list)\n",
    "size = 20\n",
    "img_width = 20\n",
    "img_height = 20\n",
    "channels = 3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale = 1 / 255 #,\n",
    "    \n",
    "    # optional data augmentation\n",
    "    #rotation_range = 40,\n",
    "    #rwidth_shift_range = 0.2,\n",
    "    #rheight_shift_range = 0.2,\n",
    "    #rshear_range = 0.2,\n",
    "    #rzoom_range = 0.2,\n",
    "    #rhorizontal_flip = TRUE,\n",
    "    #rfill_mode = \"nearest\"\n",
    ")\n",
    "\n",
    "valid_data_gen = ImageDataGenerator(\n",
    "    # validation data shouldn't be augmented\n",
    "    rescale = 1 / 255\n",
    ")\n",
    "\n",
    "train_image_array_gen = train_data_gen.flow_from_directory(\n",
    "    train_image_files_path,\n",
    "    target_size = (img_width, img_height),\n",
    "    class_mode = 'categorical',\n",
    "    classes = fruit_list,\n",
    "    color_mode = 'rgb', \n",
    "    batch_size = batch_size,\n",
    "    seed = 42)\n",
    "\n",
    "valid_image_array_gen = valid_data_gen.flow_from_directory(\n",
    "    valid_image_files_path,\n",
    "    target_size = (img_width, img_height),\n",
    "    class_mode = 'categorical',\n",
    "    classes = fruit_list,\n",
    "    color_mode = 'rgb', \n",
    "    batch_size = batch_size,\n",
    "    seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "train_samples = train_image_array_gen.n\n",
    "valid_samples = valid_image_array_gen.n\n",
    "print(train_samples, valid_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# first hidden layer\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", input_shape = (img_width, img_height, channels)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# second hidden layer\n",
    "model.add(Conv2D(16, (3, 3), padding = \"same\"))\n",
    "model.add(LeakyReLU(alpha = 0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# max pooling\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Flatten max filtered output into feature vector \n",
    "# and feed into dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Outputs from dense layer are projected onto output layer\n",
    "model.add(Dense(output_n))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = RMSprop(lr = 0.0001, decay = 1e-6),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_image_array_gen,\n",
    "    steps_per_epoch = int(train_samples / batch_size), \n",
    "    epochs = epochs, \n",
    "    validation_data = valid_image_array_gen,\n",
    "    validation_steps = int(valid_samples / batch_size),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!wget https://upload.wikimedia.org/wikipedia/commons/8/8a/Banana-Single.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_files_path = \"/keras2production/notebooks/1-deeplearning/test_images/\"\n",
    "test_images = !find $test_image_files_path -type f -name \"*.jpg\"\n",
    "print(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_image_array_gen.class_indices\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_model(image, classes=classes):\n",
    "    img = cv2.imread(image)        \n",
    "    b,g,r = cv2.split(img)       # get b,g,r\n",
    "    img = cv2.merge([r,g,b])     # switch it to rgb\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "    image = image_utils.load_img(image, target_size=(img_width, img_height))\n",
    "    image = image_utils.img_to_array(image)\n",
    "\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # scale pixels between -1 and 1, sample-wise\n",
    "    image /= 255.\n",
    "        \n",
    "    prediction = model.predict(image)\n",
    "    \n",
    "    pred = prediction.argmax()\n",
    "\n",
    "    for k, v in classes.items():\n",
    "        if (v == pred):\n",
    "            pred_label = k\n",
    "        \n",
    "    proba = prediction.max()\n",
    "    \n",
    "    print(\"Predicted class: \" + pred_label + \" with probability \" + str(proba*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_image_model(test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the net\n",
    "\n",
    "https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.4-visualizing-what-convnets-learn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image_utils.load_img(test_images[0], target_size=(20, 20))\n",
    "img_tensor = image_utils.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "# Remember that the model was trained on inputs\n",
    "# that were preprocessed in the following way:\n",
    "img_tensor /= 255.\n",
    "\n",
    "# Its shape is (1, 150, 150, 3)\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the outputs of the top layers:\n",
    "top = 7\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers[:top]]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# This will return a list of 5 Numpy arrays:\n",
    "# one array per layer activation\n",
    "activations = activation_model.predict(img_tensor)\n",
    "\n",
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:top]:\n",
    "    layer_names.append(layer.name)\n",
    "print(layer_names)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/keras2production/notebooks/fruits.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
